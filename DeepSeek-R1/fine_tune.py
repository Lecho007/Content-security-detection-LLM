import json
import os
import time
import signal
import torch
from random import sample
from datasets import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import (
    LoraConfig,
    get_peft_model,
    TaskType,
    prepare_model_for_kbit_training
)

# ========== é…ç½® ==========
model_name = "models/DeepSeek-R1-Distill-Qwen-7B"
train_path = "data/train.jsonl"
eval_path = "data/val.jsonl"
save_path = "models/deepseek-lora-output"
os.makedirs(save_path, exist_ok=True)

# ========== åŠ è½½æ•°æ® ==========
def load_jsonl_dataset(path):
    with open(path, 'r', encoding='utf-8') as f:
        lines = [json.loads(line.strip()) for line in f if line.strip()]
    return Dataset.from_list(lines)

train_dataset = load_jsonl_dataset(train_path)
eval_dataset = load_jsonl_dataset(eval_path)

# ========== åŠ è½½ tokenizer å’Œæ¨¡å‹ ==========
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    load_in_4bit=True,
    trust_remote_code=True
)

# ========== LoRA é…ç½® ==========
model = prepare_model_for_kbit_training(model)
peft_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=8,
    lora_alpha=16,
    lora_dropout=0.1,
    inference_mode=False
)
model = get_peft_model(model, peft_config)

# ========== Prompt æ„é€  & Tokenization ==========
def generate_prompt(example):
    prompt = f"{example['instruction']}\n{example['input']}".strip()
    return f"{prompt}\n{example['output']}"

def tokenize(example):
    return tokenizer(generate_prompt(example), padding="max_length", truncation=True, max_length=512)

tokenized_train = train_dataset.map(tokenize)
tokenized_eval = eval_dataset.map(tokenize)

# ========== Trainer å‚æ•° ==========
training_args = TrainingArguments(
    output_dir=save_path,
    per_device_train_batch_size=8,
    gradient_accumulation_steps=2,
    num_train_epochs=3,
    learning_rate=1e-4,
    fp16=True,
    evaluation_strategy="steps",
    eval_steps=100,
    save_strategy="steps",
    save_steps=100,
    save_total_limit=1,
    logging_steps=1,
    report_to="none",
    load_best_model_at_end=True
)

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# ========== æ ·ä¾‹å¯¹æ¯”å¯è§†åŒ–å‡½æ•° ==========
def generate_and_compare_outputs(model, tokenizer, eval_dataset, step, save_dir, num_samples=3):
    model.eval()
    samples = sample(list(eval_dataset), num_samples)
    results = []

    for item in samples:
        prompt = f"{item['instruction']}\n{item['input']}".strip()
        input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(model.device)
        with torch.no_grad():
            outputs = model.generate(
                input_ids=input_ids,
                max_new_tokens=200,
                do_sample=True,
                top_p=0.9,
                temperature=0.7,
                eos_token_id=tokenizer.eos_token_id,
                pad_token_id=tokenizer.pad_token_id
            )
        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
        generated = generated.replace(prompt, "").strip()
        results.append({
            "Prompt": prompt,
            "Reference Output": item["output"],
            "Model Output": generated
        })

    output_file = os.path.join(save_dir, f"eval_outputs_step{step}.md")
    with open(output_file, "w", encoding="utf-8") as f:
        f.write(f"# æ¨¡å‹éªŒè¯è¾“å‡ºå¯¹æ¯”ï¼ˆStep {step}ï¼‰\n\n")
        for i, result in enumerate(results):
            f.write(f"## ç¤ºä¾‹ {i+1}\n")
            f.write(f"**Prompt**:\n```\n{result['Prompt']}\n```\n")
            f.write(f"**å‚è€ƒè¾“å‡ºï¼ˆReferenceï¼‰**:\n```\n{result['Reference Output']}\n```\n")
            f.write(f"**æ¨¡å‹è¾“å‡ºï¼ˆGeneratedï¼‰**:\n```\n{result['Model Output']}\n```\n\n---\n\n")
    print(f"[ğŸ”] æ ·ä¾‹è¾“å‡ºå·²ä¿å­˜åˆ°ï¼š{output_file}")

# ========== è‡ªå®šä¹‰ Trainer ==========
class MyTrainer(Trainer):
    def training_step(self, model, inputs, num_items):
        step_start_time = time.time()
        loss = super().training_step(model, inputs, num_items)
        step_time = time.time() - step_start_time
        print(f"[Step {self.state.global_step}/{self.state.max_steps}] Loss: {loss:.4f} | Time: {step_time:.2f}s")
        return loss

    def evaluate(self, eval_dataset=None):
        print("\nRunning evaluation...")
        eval_start_time = time.time()
        eval_result = super().evaluate(eval_dataset)
        eval_time = time.time() - eval_start_time
        eval_loss = eval_result.get("eval_loss", None)
        print(f"[Eval @ Epoch {self.state.epoch:.2f}] Eval Loss: {eval_loss:.4f} | Eval Time: {eval_time:.2f}s\n")

        # ğŸ§ª æ ·ä¾‹è¾“å‡ºå¯è§†åŒ–
        generate_and_compare_outputs(self.model, self.tokenizer, self.eval_dataset, self.state.global_step, self.args.output_dir)

        return eval_result

# ========== Ctrl+C ä¸­æ–­ä¿å­˜é’©å­ ==========
trainer = MyTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_eval,
    tokenizer=tokenizer,
    data_collator=data_collator,
)

def save_on_interrupt(signal_num, frame):
    print("\nğŸ›‘ æ•è·åˆ° Ctrl+Cï¼Œæ­£åœ¨ä¿å­˜æ¨¡å‹...")
    trainer.save_model(save_path)
    tokenizer.save_pretrained(save_path)
    print("âœ… æ¨¡å‹å·²ä¿å­˜åˆ°ï¼š", save_path)
    exit(0)

signal.signal(signal.SIGINT, save_on_interrupt)

# ========== å¼€å§‹è®­ç»ƒ ==========
start_time = time.time()
print("ğŸš€ å¼€å§‹å¾®è°ƒ...\n")
trainer.train(resume_from_checkpoint=False)
end_time = time.time()
print(f"\nâœ… è®­ç»ƒå®Œæˆï¼æ€»è€—æ—¶ï¼š{end_time - start_time:.2f} ç§’ â‰ˆ {(end_time - start_time)/60:.2f} åˆ†é’Ÿ")

# ========== æœ€ç»ˆæ¨¡å‹ä¿å­˜ ==========
model.save_pretrained(save_path)
tokenizer.save_pretrained(save_path)
print(f"âœ… æœ€ç»ˆæ¨¡å‹ä¿å­˜å®Œæˆï¼š{save_path}")
