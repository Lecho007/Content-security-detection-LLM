import json
import time
import torch
from transformers import BertTokenizer, BertForSequenceClassification
from tqdm import tqdm

"""
    æ¨ç†è„šæœ¬ï¼ˆæ”¯æŒGPUã€æ˜¾ç¤ºè¿›åº¦å’Œé¢„ä¼°å‰©ä½™æ—¶é—´ï¼‰
    è¾“å…¥æ–‡ä»¶æ ¼å¼ï¼ˆJSONLï¼‰ï¼š{"text": "è¯·ç»™æˆ‘ä¸€äº›æå‡å†™ä½œæŠ€å·§çš„å»ºè®®ã€‚"}
"""

# âœ… æ¨¡å‹å’Œåˆ†è¯å™¨è·¯å¾„
MODEL_PATH = "../../models/chiness-macbert-base_output"
INPUT_JSONL = "data/test_data.jsonl"      # â† æ›¿æ¢æˆè¾“å…¥æ–‡ä»¶å
OUTPUT_JSONL = "data/inference_output.jsonl"    # â† æ¨ç†åè¾“å‡ºæ–‡ä»¶å

# âœ… è®¾å¤‡é…ç½®
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸš€ å½“å‰ä½¿ç”¨è®¾å¤‡ï¼š{device}")

# âœ… åŠ è½½æ¨¡å‹ä¸åˆ†è¯å™¨
tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)
model = BertForSequenceClassification.from_pretrained(MODEL_PATH)
model.to(device)
model.eval()

# âœ… å¯é€‰ï¼šæ ‡ç­¾æ˜ å°„
label_map = {
    "è¿æ³•çŠ¯ç½ª": 0,
    "ææ€–ä¸»ä¹‰": 1,
    "æ­§è§†åè§": 2,
    "ä¾µçŠ¯æƒç›Š": 3,
    "åé¢è¯±å¯¼": 4,
    "ç›®æ ‡åŠ«æŒ": 5,
    "Promptæ³„æ¼": 6,
    "ä¸å®‰å…¨çš„æŒ‡ä»¤ä¸»é¢˜": 7,
    "èµ‹äºˆè§’è‰²åå‘æŒ‡ä»¤": 8,
    "å¸¦æœ‰ä¸å®‰å…¨è§‚ç‚¹çš„è¯¢é—®": 9,
    "åè§æ­§è§†": 10,
    "è„è¯ä¾®è¾±": 11,
    "å¿ƒç†å¥åº·": 12,
    "èº«ä½“ä¼¤å®³": 13,
    "è´¢äº§éšç§": 14,
    "é“å¾·ä¼¦ç†": 15,
    "æ­£å¸¸æ–‡æœ¬": 16
}
# åå‘æ˜ å°„
id_to_label = {v: k for k, v in label_map.items()}

# âœ… æ¨ç†å‡½æ•°ï¼ˆæ”¯æŒGPUï¼‰
def predict(text):
    encoding = tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        padding="max_length",
        max_length=128
    )
    encoding = {k: v.to(device) for k, v in encoding.items()}
    with torch.no_grad():
        outputs = model(**encoding)
        logits = outputs.logits
        pred_id = torch.argmax(logits, dim=1).item()
    return pred_id

# âœ… ä¸»ç¨‹åº
def run_batch_inference():
    with open(INPUT_JSONL, "r", encoding="utf-8") as infile:
        lines = infile.readlines()

    total = len(lines)
    results = []
    start_time = time.time()

    for idx, line in enumerate(tqdm(lines, desc="ğŸ“¦ æ­£åœ¨æ‰¹é‡æ¨ç†ä¸­")):
        item = json.loads(line.strip())
        text = item["text"]
        label_id = predict(text)
        label_name = id_to_label.get(label_id, str(label_id))

        results.append({
            "text": text,
            "label_id": label_id,
            "label": label_name
        })

        # è¿›åº¦ä¼°è®¡æ¯10æ¡æ˜¾ç¤ºä¸€æ¬¡
        if idx % 10 == 0 or idx == total - 1:
            elapsed = time.time() - start_time
            avg_time = elapsed / (idx + 1)
            eta = avg_time * (total - (idx + 1))
            print(f"è¿›åº¦ï¼š{idx+1}/{total}ï¼Œå‰©ä½™ä¼°è®¡æ—¶é—´ï¼š{eta:.1f}s")

    # âœ… ä¿å­˜ç»“æœ
    with open(OUTPUT_JSONL, "w", encoding="utf-8") as outfile:
        for r in results:
            outfile.write(json.dumps(r, ensure_ascii=False) + "\n")

    print(f"\nâœ… æ¨ç†å®Œæˆï¼Œç»“æœä¿å­˜åœ¨ï¼š{OUTPUT_JSONL}")

if __name__ == "__main__":
    run_batch_inference()
